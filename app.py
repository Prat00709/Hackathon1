# -*- coding: utf-8 -*-
"""Untitled25.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e_-pMoYlurp1-aNT1HuuLohw9ZHiSbT1
"""

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC

st.set_page_config(layout="wide")
st.title("PCOS Diagnosis Model Comparison Dashboard")

# Load dataset
@st.cache_data
def load_data():
    return pd.read_csv("pcos_dataset.csv")

df = load_data()
st.subheader("Raw Dataset")
st.dataframe(df.head())

# Features and target
X = df.drop("PCOS_Diagnosis", axis=1)
y = df["PCOS_Diagnosis"]

# Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Models to train
models = {
    "Logistic Regression": LogisticRegression(),
    "Decision Tree": DecisionTreeClassifier(),
    "Random Forest": RandomForestClassifier(),
    "SVM": SVC(),
    "Gradient Boosting": GradientBoostingClassifier()
}

# Evaluation metrics
def evaluate_model(name, model, X_test, y_test):
    y_pred = model.predict(X_test)
    return {
        "Model": name,
        "Accuracy": accuracy_score(y_test, y_pred),
        "Precision": precision_score(y_test, y_pred),
        "Recall": recall_score(y_test, y_pred),
        "F1 Score": f1_score(y_test, y_pred)
    }

# Train and evaluate models
results = []
with st.spinner("Training baseline models..."):
    for name, model in models.items():
        model.fit(X_train, y_train)
        results.append(evaluate_model(name, model, X_test, y_test))

results_df = pd.DataFrame(results)
st.subheader("Model Performance (Default Parameters)")
st.dataframe(results_df)

# Visualization
st.subheader("Performance Comparison (Default)")
melted = results_df.melt(id_vars="Model", var_name="Metric", value_name="Score")
fig, ax = plt.subplots(figsize=(12, 6))
sns.barplot(data=melted, x="Model", y="Score", hue="Metric", ax=ax)
plt.xticks(rotation=45)
plt.tight_layout()
st.pyplot(fig)

# Show correlation matrix
st.subheader("Correlation Matrix")
corr_matrix = df.corr(numeric_only=True)
fig_corr, ax_corr = plt.subplots(figsize=(12, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5, ax=ax_corr)
st.pyplot(fig_corr)

# Option to show tuned results
if st.checkbox("Show Tuned Models Performance"):
    tuned_models = {
        "Logistic Regression": GridSearchCV(LogisticRegression(), {"C": [0.1, 1, 10]}, cv=5),
        "Decision Tree": GridSearchCV(DecisionTreeClassifier(), {"max_depth": [3, 5, 7]}, cv=5),
        "Random Forest": GridSearchCV(RandomForestClassifier(), {"n_estimators": [50, 100], "max_depth": [5, 10]}, cv=5),
        "SVM": GridSearchCV(SVC(), {"C": [0.1, 1, 10], "kernel": ["linear", "rbf"]}, cv=5),
        "Gradient Boosting": GridSearchCV(GradientBoostingClassifier(), {"n_estimators": [50, 100], "learning_rate": [0.01, 0.1]}, cv=5)
    }

    tuned_results = []
    with st.spinner("Tuning hyperparameters and retraining models..."):
        for name, search in tuned_models.items():
            search.fit(X_train, y_train)
            best_model = search.best_estimator_
            tuned_results.append(evaluate_model(name + " (Tuned)", best_model, X_test, y_test))

    tuned_results_df = pd.DataFrame(tuned_results)
    st.subheader("Model Performance (After Hyperparameter Tuning)")
    st.dataframe(tuned_results_df)

    final_results = pd.concat([results_df, tuned_results_df], ignore_index=True)
    st.subheader("Final Comparison (Default + Tuned)")
    final_melted = final_results.melt(id_vars="Model", var_name="Metric", value_name="Score")
    fig_final, ax_final = plt.subplots(figsize=(12, 6))
    sns.barplot(data=final_melted, x="Model", y="Score", hue="Metric", ax=ax_final)
    plt.xticks(rotation=45)
    plt.tight_layout()
    st.pyplot(fig_final)

st.success("Dashboard Ready!")